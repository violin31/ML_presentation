{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d902977d-a741-4b65-a2f7-cf77a58cd568",
   "metadata": {},
   "source": [
    "# ML and spectroscopy: toy applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fca5a-421f-47d1-a254-dcaa91886366",
   "metadata": {},
   "source": [
    "## Diatomic constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed492e9-0d07-4620-b0c5-65d521e4b96c",
   "metadata": {},
   "source": [
    "We are implemementing a ML where a Random Forest model is used to predict the rovibrational energy level of CO , and then a Linear Regression model is used to approximate the molecular constants. The model performance is evaluated using Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R$^2$.\n",
    "\n",
    "Our dataset is originated using the equation: \n",
    "\n",
    "$$ E_{\\text{rovib, anharmonic}}(v, J) = T_e +  \\left( v + \\frac{1}{2} \\right) \\omega_e - \\left( v + \\frac{1}{2} \\right)^2 \\omega_e x_e  + \\left( B  - D_e J(J+1) \\right) J(J+1) + \\alpha_e J^2(J+1)^2$$\n",
    "\n",
    "Where:\n",
    "- $ T_e$ is the **electronic energy**.\n",
    "- $ v $ is the **vibrational quantum number**.\n",
    "- $ \\omega_e$ is the **harmonic vibrational frequency**.\n",
    "- $ \\omega_e x_e $ is the **anharmonic correction** to the vibrational frequency.\n",
    "- $ B $ is the **rotational constant** for the ground vibrational state (first rotational constant).\n",
    "- $ D_e $ is the **centrifugal distortion constant**, which corrects the rotational energy levels for the elongation of the bond during rotation.\n",
    "- $ J(J+1) $ is the **rotational energy term**, where \\( J \\) is the rotational quantum number.\n",
    "- $ \\alpha_e$ is the change in the rotational constant due to vibrational excitation. \n",
    "\n",
    "\n",
    "\n",
    "This exercise evaluates the model performance when the analytical data are perturbed  and recovers the  polynomial coefficients using Linear Regression after such perturbation. The scope is to provide insights into how well the Random Forest model learned the underlying structure. Molecular constants are taken from the [NIST webbook](https://webbook.nist.gov/cgi/cbook.cgi?ID=C630080&Mask=1000#Diatomic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b6ff5-b809-4fb7-a22b-5fc0c2d30e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install seaborn\n",
    "\n",
    "\n",
    "!rm *.txt*\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook2/CO.txt\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook2/h2o.txt\n",
    "!wget https://raw.githubusercontent.com/marcpezz/ML_presentation/refs/heads/main/Notebook2/d2o.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63ebc2-60b0-4c21-a7ba-f651bfc8bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score,root_mean_squared_error\n",
    "\n",
    "def generate_data(num_samples=300):\n",
    "    true_coefficients = [0.0,2169.81358,13.28831,1.93128087,0.01750441,6.12147e-06]  # Example coefficients for a0, a1, a2, a3, a4\n",
    "    X1 = np.random.randint(0, 30, size=num_samples)\n",
    "    X2 = np.random.randint(0, 30, size=num_samples)\n",
    "    \n",
    "    # Calculate output based on the new formula\n",
    "    y = (true_coefficients[0] + \n",
    "         true_coefficients[1] * (X1 + 0.5) + \n",
    "         true_coefficients[2] * (X1 + 0.5)**2 + \n",
    "         (true_coefficients[3] + true_coefficients[5] * (X1 + 0.5)) * X2 * (X2 + 1) + \n",
    "         true_coefficients[4] * (X2**2) * (X2 + 1)**2)\n",
    "    \n",
    "    y += np.random.normal(0, 0.1, num_samples)  # Adding noise\n",
    "    return np.column_stack((X1, X2)), y\n",
    "\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data(num_samples=400)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=1000)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf_est = rf_model.predict(X_test)\n",
    "\n",
    "# Step 4: Make predictions with the Random Forest model\n",
    "y_pred_rf = rf_model.predict(X_train)\n",
    "\n",
    "# Step 5: Fit a linear regression model to approximate the coefficients\n",
    "# Create transformed features based on the original function\n",
    "X_transformed = np.column_stack([\n",
    "    (X_train[:, 0] + 0.5), \n",
    "    (X_train[:, 0] + 0.5) ** 2,\n",
    "    X_train[:, 1] * (X_train[:, 1] + 1),\n",
    "    (X_train[:, 1] ** 2) * (X_train[:, 1] + 1) ** 2\n",
    "])\n",
    "\n",
    "# Fit the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_transformed, y_pred_rf)\n",
    "\n",
    "# Print the coefficients\n",
    "predicted_coefficients = linear_model.coef_\n",
    "print(f\"Predicted Coefficients Train test:\")\n",
    "print(f\"a1: {predicted_coefficients[0]}\")\n",
    "print(f\"a2: {predicted_coefficients[1]}\")\n",
    "print(f\"a3: {predicted_coefficients[2]}\")\n",
    "print(f\"a4: {predicted_coefficients[3]}\")\n",
    "print(f\"    \")\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "# Step 5: Fit a linear regression model to approximate the coefficients\n",
    "# Create transformed features based on the original function\n",
    "X_transformed2 = np.column_stack([\n",
    "    (X_test[:, 0] + 0.5), \n",
    "    (X_test[:, 0] + 0.5) ** 2,\n",
    "    X_test[:, 1] * (X_test[:, 1] + 1),\n",
    "    (X_test[:, 1] ** 2) * (X_test[:, 1] + 1) ** 2\n",
    "])\n",
    "\n",
    "# Fit the linear regression model\n",
    "linear_model2 = LinearRegression()\n",
    "linear_model2.fit(X_transformed2, y_pred_rf_est)\n",
    "\n",
    "# Print the coefficients\n",
    "predicted_coefficients2 = linear_model2.coef_\n",
    "print(f\"Predicted Coefficients Test set:\")\n",
    "print(f\"a1: {predicted_coefficients2[0]}\")\n",
    "print(f\"a2: {predicted_coefficients2[1]}\")\n",
    "print(f\"a3: {predicted_coefficients2[2]}\")\n",
    "print(f\"a4: {predicted_coefficients2[3]}\")\n",
    "print(f\"    \")\n",
    "#############################################################################################################\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_rf_est)\n",
    "rmse= root_mean_squared_error(y_test, y_pred_rf_est)\n",
    "r2 = r2_score(y_test, y_pred_rf_est)\n",
    "\n",
    "print(f\"Estimation metrics:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "\n",
    "# Visualise predictions \n",
    "plt.xlim(0,24000)\n",
    "plt.ylim(0,24000)\n",
    "plt.plot([0, 24000], [0, 24000], color='red', linestyle='--')\n",
    "plt.plot( y_train,y_pred_rf,\"o\")\n",
    "plt.plot( y_test,y_pred_rf_est,\"o\")\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "true_coefficients = [0.0,2169.81358,13.28831,1.93128087,0.01750441,6.12147e-06]  # Example coefficients for a0, a1, a2, a3, a4\n",
    "\n",
    "print(f\"Difference in coefs /ratio\")\n",
    "print(f\"a1: {(true_coefficients[1] - predicted_coefficients2[0])/true_coefficients[1]}\")\n",
    "print(f\"a2: {(true_coefficients[2] - predicted_coefficients2[1])/true_coefficients[2]}\")\n",
    "print(f\"a3: {(true_coefficients[3] - predicted_coefficients2[2])/true_coefficients[3]}\")\n",
    "print(f\"a4: {(true_coefficients[4] - predicted_coefficients2[3])/true_coefficients[4]}\")\n",
    "print(f\"    \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ecd40-c417-4d2d-b691-4ad974812e94",
   "metadata": {},
   "source": [
    "we are attempting to apply Random Forest Regression to predict molecular energy levels  from the [ExoMol database](https://exomol.com/data/molecules/CO/12C-16O/Li2015/) based on the quantum numbers v and J and then use Linear Regression to approximate the coefficients of polynomial transformations of these features. \n",
    "\n",
    "**NB:** The ExoMol states file is in the following format. (Old format, see the [most recent paper](https://doi.org/10.48550/arXiv.2406.06347) for the formatting of more recent data - forehsadowing)\n",
    "| Field                  | Description                                                                 |\n",
    "|------------------------|-----------------------------------------------------------------------------|\n",
    "| `i`                    | State ID                                                                    |\n",
    "| $\\tilde{E}$            | Recommended state energy in $\\mathrm{cm^{-1}}$                              |\n",
    "| $g_\\mathrm{tot}$       | State degeneracy                                                            |\n",
    "| $J$                    | Total angular momentum quantum number, $J$ or $F$ (integer/half-integer)    |\n",
    "| Unc                    | Uncertainty in the state energy in $\\mathrm{cm^{-1}}$                       |\n",
    "| $\\tau$                 | State lifetime (aggregated radiative and predissociative lifetimes) in s    |\n",
    "| ($g$)                  | Landé $g$-factor (optional)                                                 |\n",
    "| (QN)                   | State quantum numbers, may be several columns (optional)                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76a4d4-fc57-4ae2-aa9b-347610810666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CO=pd.read_csv(\"CO.txt\",names=[\"idx\",\"E\",\"gns\",\"J\",\"v\",\"e/f\"],sep=\"\\s+\")\n",
    "X = CO[['v', 'J']].values\n",
    "Y = CO['E'].values\n",
    "\n",
    "for i in range(1,3):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "    rf_model = RandomForestRegressor(n_estimators=100)#,criterion=\"squared_error\",oob_score=True,random_state=10)\n",
    "    rf_model.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred_rf_est = rf_model.predict(X_test)\n",
    "    y_pred_rf = rf_model.predict(X_train)\n",
    "\n",
    "    X_transformed = np.column_stack([\n",
    "        (X_test[:, 0] + 0.5), \n",
    "        (X_test[:, 0] + 0.5) ** 2,\n",
    "        (X_test[:, 1] * (X_test[:, 1] + 1) * (X_test[:, 0] + 0.5)),  # (a3 + a4 * (x1 + 0.5)) * x2 * (x2 + 1)\n",
    "        (X_test[:, 1] ** 2) * (X_test[:, 1] + 1) ** 2  # a5 * (x2^2) * (x2 + 1)^2\n",
    "    ])\n",
    "# Fit the linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_transformed, y_pred_rf_est)\n",
    "    predicted_coefficients = linear_model.coef_\n",
    "#\n",
    "#predicted_coefficients = linear_model.coef_\n",
    "    print(f\"Predicted Coefficients:\")\n",
    "    print(f\"a1: {predicted_coefficients[0]}\")\n",
    "    print(f\"a2: {predicted_coefficients[1]}\")\n",
    "    print(f\"a3: {predicted_coefficients[2]}\")\n",
    "    print(f\"a4: {predicted_coefficients[3]}\")\n",
    "    print(f\"    \")\n",
    "\n",
    "    print(f\"Difference in coefs /ratio\")\n",
    "    print(f\"a1: {(true_coefficients[1] - predicted_coefficients[0])/true_coefficients[1]}\")\n",
    "    print(f\"a2: {(true_coefficients[2] - np.abs(predicted_coefficients[1]))/true_coefficients[2]}\")\n",
    "    print(f\"a3: {(true_coefficients[3] - np.abs(predicted_coefficients[2]))/true_coefficients[3]}\")\n",
    "    print(f\"a4: {(true_coefficients[4] - np.abs(predicted_coefficients[3]))/true_coefficients[4]}\")\n",
    "   #print(f\"Root Mean Squared Error: {np.sqrt(mse)}\")\n",
    "    #print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Optional: Visualize predictions (same as before)\n",
    "\n",
    "plt.xlim(0,15000)\n",
    "plt.ylim(0,15000)\n",
    "plt.plot([0, 15000], [0, 15000], color='red', linestyle='--')\n",
    "plt.plot( Y_test,y_pred_rf_est,\"o\")\n",
    "\n",
    "#plt.plot( y_test,\"o\")\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d543f-b2c9-435e-b50c-598a9aa492bd",
   "metadata": {},
   "source": [
    "In the following part, we use the polynomial regression and the PolynomialFeatures and LinearRegression classes from the scikit-learn library. We perform the  regression to predict the diatomic constants polynomial model and then extracting certain coefficients from the fitted polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849ebbf-83f1-4a5c-aa30-63995251e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Step 2: Transform features\n",
    "degree = 4  # Change this to fit a different degree polynomial\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly.fit_transform(X_test)\n",
    "\n",
    "# Step 3: Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_pred_rf_est)\n",
    "predicted_coefficients = model.coef_\n",
    "\n",
    "we=predicted_coefficients[1]\n",
    "wexe=np.abs(predicted_coefficients[3])\n",
    "Be=np.abs(predicted_coefficients[5])\n",
    "ae=np.abs(predicted_coefficients[8])\n",
    "De=np.abs(predicted_coefficients[14])\n",
    "\n",
    "print(we,wexe,Be,De,ae,true_coefficients)\n",
    "print(f\"Difference in coefs /ratio\")\n",
    "print(f\"we: {(true_coefficients[1] - we)/true_coefficients[1]}\")\n",
    "print(f\"wexe: {(true_coefficients[2] - wexe)/true_coefficients[2]}\")\n",
    "print(f\"BE: {(true_coefficients[3] - Be)/true_coefficients[3]}\")\n",
    "print(f\"ae: {(true_coefficients[4] - ae)/true_coefficients[4]}\")\n",
    "print(f\"De: {(true_coefficients[5] - De)/true_coefficients[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a519f0-4959-4570-90b1-720252aaeca9",
   "metadata": {},
   "source": [
    "## Prediciting the water energy levels Using  Scikit-Learn + Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f2e11-5397-44ce-a86a-abf0fe8ac3ec",
   "metadata": {},
   "source": [
    "The key quantum numbers that describe water's rotational energy levels are $v_1$, $v_2$, $v_3$, $J$, $K_a$, and $K_c$.\n",
    "\n",
    "The **vibrational quantum numbers** include $v_1$, which refers to the symmetric stretching mode where the hydrogen atoms move symmetrically towards or away from the oxygen atom, $v_2$, which corresponds to the bending motion of the H-O-H bond angle, and $v_3$, which describes the asymmetric stretching mode, where one hydrogen atom moves towards the oxygen while the other moves away. For the **rotational quantum numbers**, $J$ represents the total rotational angular momentum of the molecule. $K_a$ and $K_c$ are projections of $J$ onto the principal axes of inertia, where $K_a$ refers to the projection along the axis with the intermediate moment of inertia, and $K_c$ refers to the projection along the axis with the smallest moment of inertia.\n",
    "\n",
    "Obtaining energy levels and their corresponding quantum numbers is a challenging task that involves numerous experiments and calculations. These difficulties derives from the accuracy of the *ab initio* calculations required to solve the nuclear Schrödinger equation, which is necessary for determining the energy levels. Spectroscopy experiments provide the transition energies between different levels and their associated quantum numbers. By using combination differences on these transitions, the energy levels can then be assigned.\n",
    "\n",
    "In this tutorial, we will explore the possibility of predicting the energy levels of molecules using the quantum numbers as inputs and the energy levels as outputs. The main goal is to produce a model that will be within chemical accuracy (below 350 cm$^{-1}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c4cbc-cb4e-4e1f-8a7e-873353d7388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "l1=512\n",
    "l2=256\n",
    "size=0.2\n",
    "dropout=0.2\n",
    "rate=0.0005\n",
    "l2norm=0.0002\n",
    "epoch=100\n",
    "wn2h=8065.54\n",
    "#wn2h=219474.6313708\n",
    "#wn2h=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e8de9-a87d-46c8-ae6b-997d79339ebe",
   "metadata": {},
   "source": [
    "We will use scikit learn for the data preprocessing and tensorflow for the Neural Network. We will try to see what happens if we are using three different units: eV, Hartree or wavenumbes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81fe62-1aab-4bde-a19e-db1e24117424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the data from a file\n",
    "h2o = pd.read_csv('h2o.txt', sep=\"\\s+\")\n",
    "h2o.drop(\"r\",axis=1)\n",
    "# Separate inputs (X), target (Y), and uncertainty (U)\n",
    "X = h2o[['v1', 'v2', 'v3', 'J', 'Ka', 'Kc']].values\n",
    "Y = h2o['E'].values\n",
    "U = h2o['U'].values\n",
    "\n",
    "Y=Y/wn2h\n",
    "U=U/wn2h\n",
    "\n",
    "# Handle zero uncertainty values: Set a minimum threshold to avoid division by zero\n",
    "U[U == 0] = 1e-10  # You can choose a small value to replace zero uncertainty\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_tmp, Y_train, Y_tmp, U_train, U_tmp = train_test_split(X, Y, U, test_size=0.2)#, random_state=42)\n",
    "X_valid, X_test, Y_valid, Y_test, U_valid, U_test = train_test_split(X_tmp, Y_tmp, U_tmp, test_size=0.5)#, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Define a custom loss function that incorporates uncertainty\n",
    "def weighted_mse_loss(y_true, y_pred, uncertainty):\n",
    "    return tf.reduce_mean(((y_true - y_pred) ** 2) / (uncertainty ** 2))\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(l1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2norm)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Dense(l2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2norm)),\n",
    "    tf.keras.layers.Dense(1,activation='relu')#,activation='softmax')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with a specified learning rate and custom loss function\n",
    "learning_rate = rate\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "print(\"Intra-op parallelism threads: \", tf.config.threading.get_intra_op_parallelism_threads())\n",
    "print(\"Inter-op parallelism threads: \", tf.config.threading.get_inter_op_parallelism_threads())\n",
    "# Train the model and pass sample weights\n",
    "history=model.fit(X_train_scaled, Y_train, epochs=100, validation_data=(X_valid_scaled, Y_valid),callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate overall MSE and MAE\n",
    "overall_mse = mean_squared_error(Y_test, predictions)\n",
    "overall_mae = mean_absolute_error(Y_test, predictions)\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall MSE: {overall_mse}\")\n",
    "print(f\"Overall RMSE: {np.sqrt(overall_mse)}\")\n",
    "print(f\"Overall MAE: {overall_mae}\")\n",
    "\n",
    "# Compute Pearson correlation between the true values and the predictions\n",
    "correlation, p_value = pearsonr(Y_test, predictions.ravel())\n",
    "\n",
    "# Print Pearson correlation\n",
    "print(f\"Pearson Correlation: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47206e-304e-46bf-878b-03ee7cca7063",
   "metadata": {},
   "source": [
    "Plotting the Loss function as function of the Epoch helps to visualise possible overfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9a9e8-49bb-4a99-aff4-5a07b0b154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "steps = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.plot(steps, loss, label='Training Loss')\n",
    "plt.plot(steps, val_loss, label='Validation Loss')\n",
    "plt.title('Loss Function as Number of Steps')\n",
    "plt.xlabel('Training Steps (Epochs)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64d1b0-a2bb-469a-b027-ff353d4a05fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h2o=h2o.drop(\"r\",axis=1)\n",
    "h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae5d8b-f3f1-4cee-a629-3fe1b1cf34e3",
   "metadata": {},
   "source": [
    "**Skewness** is a statistical measure that quantifies the *asymmetry of the probability distribution* of a real-valued random variable about its mean. In the context of machine learning errors, skewness can provide insights into how the errors are distributed relative to the mean error.\n",
    "\n",
    "**Kurtosis** is a statistical measure that describes the *shape of a probability distribution*, particularly the tails and the sharpness of the peak. In the context of machine learning errors, kurtosis helps understand the distribution's extremity, specifically how concentrated the values are around the mean and the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c7130-5524-498a-ab0b-b12a804483cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import scipy\n",
    "# Calculate errors\n",
    "errors = Y_test -predictions.flatten()\n",
    "plt.hist(errors,bins=100)\n",
    "# Plotting the error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate and display statistics\n",
    "mean_error = np.mean(errors)\n",
    "std_error = np.std(errors)\n",
    "\n",
    "# Print statistics\n",
    "print(f'Mean Error: {mean_error:.2f}')\n",
    "print(f'Standard Deviation of Error: {std_error:.2f}')\n",
    "print(f'Skewness of Error: {scipy.stats.skew(errors):.2f}')\n",
    "print(f'Kurtosis of Error: {scipy.stats.kurtosis(errors):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76181f0-3940-4572-9226-b4301981e4bb",
   "metadata": {},
   "source": [
    "Small excursus, is there any form of info we can take trying to clustering the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520177e-52d4-4a14-bbea-20b42078a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optionally, scale your features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(h2o)\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters =5  # Set this to the desired number of clusters\n",
    "\n",
    "# Create and fit the model\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Create and fit the model\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the labels to the original DataFrame\n",
    "h2o['Cluster'] = labels\n",
    "\n",
    "# Get the cluster centers (centroids)\n",
    "print(\"Cluster Centers (Centroids):\\n\", kmeans.cluster_centers_)\n",
    "\n",
    "# Check which cluster each data point belongs to\n",
    "print(\"Cluster labels for each point:\", kmeans.labels_)\n",
    "\n",
    "# For a specific point, you can check its cluster\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a scatter plot for the first two features\n",
    "#plt.xscale(\"log\")\n",
    "#plt.yscale(\"log\")\n",
    "#plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=h2o['J'], y=h2o['E'], hue=h2o['Cluster'], palette='viridis')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('J')\n",
    "plt.ylabel('Energy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d84fc-1a2a-40a6-a9bd-cc62ebf9d4d1",
   "metadata": {},
   "source": [
    "## Isotope shift predicition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac55a6-9400-47d4-8d9a-b94bd38b1ae4",
   "metadata": {},
   "source": [
    "Download the states file for an isotope of water, match the energy levels and calculate the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960b0b0-5ca4-4747-9f75-77939fe4562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the data from a file\n",
    "d2o = pd.read_csv('d2o.txt',sep=',')\n",
    "h2o[\"E\"]=h2o[\"E\"]/wn2h\n",
    "d2o[\"E\"]=d2o[\"E\"]/wn2h\n",
    "water_intermediate = pd.merge(h2o, d2o, on=['v1', 'v2', 'v3','J','Ka','Kc'], suffixes=('_h2o', '_d2o'))\n",
    "water_intermediate['delta'] =  water_intermediate['E_h2o'] - water_intermediate['E_d2o'] \n",
    "water = water_intermediate.drop(columns=['U','n1','n2','unc'])\n",
    "#water = water[water['delta'] >= 0]\n",
    "#plt.plot(water['delta'],'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a9f37-6c1f-428a-91bb-db0f6efeb0cd",
   "metadata": {},
   "source": [
    "We are usig delta as our target variable and C as our control variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9fcdb-11ec-45c9-9006-d5b36f60f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate inputs (X), target (Y),  and control (C)\n",
    "X = water[['v1', 'v2', 'v3', 'J', 'Ka', 'Kc','E_h2o']].values\n",
    "Y = water['delta'].values\n",
    "C = water['E_d2o'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_tmp, Y_train, Y_tmp, C_train, C_tmp = train_test_split(X, Y, C,  test_size=0.2)\n",
    "X_test, X_valid, Y_test, Y_valid, C_test, C_valid = train_test_split(X_tmp, Y_tmp, C_tmp,  test_size=0.1)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(1,activation=\"relu\")  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with a specified learning rate and custom loss function\n",
    "learning_rate = 0.001\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model and pass sample weights\n",
    "history=model.fit(X_train_scaled, Y_train, epochs=100, validation_data=(X_valid_scaled, Y_valid),callbacks=[early_stopping]) \n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate overall MSE and MAE\n",
    "overall_mse = mean_squared_error(Y_test, predictions)\n",
    "overall_mae = mean_absolute_error(Y_test, predictions)\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall MSE: {overall_mse}\")\n",
    "print(f\"Overall MAE: {overall_mae}\")\n",
    "\n",
    "# Compute Pearson correlation between the true values and the predictions\n",
    "correlation, p_value = pearsonr(Y_test, predictions.ravel())\n",
    "\n",
    "# Print Pearson correlation\n",
    "print(f\"Pearson Correlation: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa5a6c-fa69-4824-b85f-6d41be2bda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "steps = range(1, len(loss) + 1)\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.plot(steps, loss, label='Training Loss')\n",
    "plt.plot(steps, val_loss, label='Validation Loss')\n",
    "plt.title('Loss Function as Number of Steps')\n",
    "plt.xlabel('Training Steps (Epochs)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8900a-6452-4424-aa88-8a26a92ce7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "# Calculate errors\n",
    "errors = Y_test -predictions.flatten()\n",
    "\n",
    "# Plotting the error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors.flatten(),bins=100)\n",
    "\n",
    "print(f'Mean Error: {mean_error:.2f}')\n",
    "print(f'Standard Deviation of Error: {std_error:.2f}')\n",
    "print(f'Skewness of Error: {scipy.stats.skew(errors):.2f}')\n",
    "print(f'Kurtosis of Error: {scipy.stats.kurtosis(errors):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f248526-9ab4-46a5-be3a-ed2f7c15bde6",
   "metadata": {},
   "source": [
    "Plot the predicted values vs the control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dd4a0-01bb-4acb-a54b-43789e0de9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'E_h2o' is the first column (index 0) of X_valid\n",
    "E_h2o_valid = X_valid[:, 6] \n",
    "\n",
    "# Step 2: Add Y_valid to E_h2o_valid\n",
    "X_valid_sum = E_h2o_valid - Y_valid\n",
    "\n",
    "# Step 3: Plot C_valid against X_valid_sum\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "plt.plot([0,3],[0,3],\"--\",color=\"red\")\n",
    "plt.plot(X_valid_sum, C_valid, 'o', label=\"C_valid vs X_valid['E_h2o'] + Y_valid\")\n",
    "plt.xlabel(\"X_valid - Y_valid\")\n",
    "plt.ylabel(\"C_valid\")\n",
    "plt.title(\"Plot of C_valid vs X_valid['E_h2o'] - Y_valid\")\n",
    "#plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e136f-fb72-4ddb-8bb3-47b27d74724f",
   "metadata": {},
   "source": [
    "Plot the H$_2$O energy vs D$_2$O energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2f197-050d-4f5a-8e6f-cdd607b7c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(water[\"E_h2o\"],water[\"E_d2o\"], \"o\")\n",
    "plt.xlabel(\"E$_{H_2O}$\")\n",
    "plt.ylabel(\"E$_{D_2O}$\")\n",
    "plt.plot([0,3],[0,3],\"--\",color=\"red\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
